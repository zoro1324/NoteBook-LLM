{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  AI Pipeline Walkthrough: From Document to Insight\n",
                "\n",
                "This notebook demonstrates the exact AI workflow used in the **NotebookLLM Clone** application. It runs standalone (without the Django backend) but uses the **same libraries and logic** to show how your data is processed.\n",
                "\n",
                "### ðŸš€ The Pipeline\n",
                "1.  **Ingestion & OCR**: Extract text from PDFs using **Doctr** (Deep Learning OCR).\n",
                "2.  **Chunking**: Split text into manageable overlapping segments.\n",
                "3.  **Embeddings**: Convert text to vectors using **SentenceTransformers** (`BAAI/bge-small-en-v1.5`).\n",
                "4.  **Vector Store**: Index vectors using **FAISS** for fast similarity search.\n",
                "5.  **Generation**: Retrieve context and answer questions using **Ollama** (`phi3:mini` or configured model)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: gTTS in d:\\notebook llm\\venv\\lib\\site-packages (2.5.4)\n",
                        "Requirement already satisfied: requests<3,>=2.27 in d:\\notebook llm\\venv\\lib\\site-packages (from gTTS) (2.32.5)\n",
                        "Requirement already satisfied: click<8.2,>=7.1 in d:\\notebook llm\\venv\\lib\\site-packages (from gTTS) (8.1.8)\n",
                        "Requirement already satisfied: colorama in d:\\notebook llm\\venv\\lib\\site-packages (from click<8.2,>=7.1->gTTS) (0.4.6)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\notebook llm\\venv\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in d:\\notebook llm\\venv\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\notebook llm\\venv\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2.6.3)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in d:\\notebook llm\\venv\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2026.1.4)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 24.0 -> 26.0\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "# ðŸ“¦ Install Dependencies\n",
                "# Run this cell if you haven't installed gTTS yet\n",
                "!pip install gTTS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "d:\\NoteBook LLM\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Using Device: CUDA\n"
                    ]
                }
            ],
            "source": [
                "# ðŸ“¦ Imports & Configuration\n",
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import numpy as np\n",
                "import faiss\n",
                "import requests\n",
                "import json\n",
                "from pathlib import Path\n",
                "from typing import List, Tuple, Dict\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# âš™ï¸ Configuration (Mirroring backend settings)\n",
                "EMBEDDING_MODEL_NAME = 'BAAI/bge-small-en-v1.5'\n",
                "OLLAMA_BASE_URL = 'http://localhost:11434'\n",
                "OLLAMA_MODEL = 'phi3:mini'  # Change if you use a different model in Ollama\n",
                "CHUNK_SIZE = 512\n",
                "CHUNK_OVERLAP = 50\n",
                "\n",
                "# Check for GPU\n",
                "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"âœ… Using Device: {DEVICE.upper()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Document Processing (OCR)\n",
                "\n",
                "We use **Doctr** (Document Text Recognition) to extract text. This is a powerful Deep Learning-based OCR engine that handles complex layouts better than standard text extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize OCR Model (Cached)\n",
                "ocr_model = None\n",
                "\n",
                "def get_ocr_model():\n",
                "    global ocr_model\n",
                "    if ocr_model is None:\n",
                "        print(\"â³ Loading Doctr OCR model... (this may take a moment)\")\n",
                "        try:\n",
                "            from doctr.models import ocr_predictor\n",
                "            ocr_model = ocr_predictor(pretrained=True).to(DEVICE)\n",
                "            print(\"âœ… OCR Model Loaded!\")\n",
                "        except ImportError:\n",
                "            print(\"âŒ Doctr not installed. Please install it to use OCR features.\")\n",
                "    return ocr_model\n",
                "\n",
                "def extract_text_from_pdf(file_path: str) -> str:\n",
                "    \"\"\"Extract text from PDF using Doctr (matches backend strategy)\"\"\"\n",
                "    model = get_ocr_model()\n",
                "    if not model:\n",
                "        return \"[Error: OCR Unavailable]\"\n",
                "\n",
                "    print(f\"ðŸ” Processing {file_path}...\")\n",
                "    from doctr.io import DocumentFile\n",
                "    \n",
                "    try:\n",
                "        # Load PDF\n",
                "        doc = DocumentFile.from_pdf(file_path)\n",
                "        # Run Inference\n",
                "        result = model(doc)\n",
                "        \n",
                "        # Reconstruct Text\n",
                "        full_text = []\n",
                "        for page_idx, page in enumerate(result.pages):\n",
                "            page_text = []\n",
                "            for block in page.blocks:\n",
                "                for line in block.lines:\n",
                "                    line_text = \" \".join([word.value for word in line.words])\n",
                "                    page_text.append(line_text)\n",
                "            \n",
                "            # Add page marker\n",
                "            full_text.append(f\"--- Page {page_idx + 1} ---\\n\" + \"\\n\".join(page_text))\n",
                "        \n",
                "        return \"\\n\\n\".join(full_text)\n",
                "    except Exception as e:\n",
                "        return f\"âŒ Error extracting text: {str(e)}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Semantic Chunking\n",
                "\n",
                "LLMs have context limits. We split the text into overlapping chunks to ensure we can retrieve just the relevant parts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:\n",
                "    \"\"\"Split text into overlapping chunks\"\"\"\n",
                "    words = text.split()\n",
                "    chunks = []\n",
                "    \n",
                "    current_chunk = []\n",
                "    current_len = 0\n",
                "    \n",
                "    for word in words:\n",
                "        word_len = len(word) + 1\n",
                "        if current_len + word_len > chunk_size and current_chunk:\n",
                "            chunks.append(\" \".join(current_chunk))\n",
                "            # Overlap: keep last 'overlap' words for next chunk\n",
                "            current_chunk = current_chunk[-overlap:]\n",
                "            current_len = sum(len(w) + 1 for w in current_chunk)\n",
                "        \n",
                "        current_chunk.append(word)\n",
                "        current_len += word_len\n",
                "    \n",
                "    if current_chunk:\n",
                "        chunks.append(\" \".join(current_chunk))\n",
                "    \n",
                "    print(f\"âœ‚ï¸  Created {len(chunks)} chunks from document.\")\n",
                "    return chunks"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Embeddings & Vector Store (RAG Core)\n",
                "\n",
                "We convert chunks into **Vector Embeddings** (lists of numbers representing meaning) and store them in **FAISS**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â³ Loading Embedding Model: BAAI/bge-small-en-v1.5...\n",
                        "âœ… Embedding Model Loaded!\n"
                    ]
                }
            ],
            "source": [
                "# Load Embedding Model\n",
                "print(f\"â³ Loading Embedding Model: {EMBEDDING_MODEL_NAME}...\")\n",
                "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
                "print(\"âœ… Embedding Model Loaded!\")\n",
                "\n",
                "class VectorStore:\n",
                "    def __init__(self, dimension=384):\n",
                "        # Using FAISS IndexFlatIP (Inner Product) for Cosine Similarity\n",
                "        # (Requires normalized vectors)\n",
                "        self.index = faiss.IndexFlatIP(dimension)\n",
                "        self.chunks = [] # Store actual text to retrieve later\n",
                "    \n",
                "    def add_documents(self, text_chunks: List[str]):\n",
                "        # 1. Embed\n",
                "        embeddings = embedder.encode(text_chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
                "        \n",
                "        # 2. Add to Index\n",
                "        self.index.add(embeddings)\n",
                "        self.chunks.extend(text_chunks)\n",
                "        print(f\"ðŸ’¾ Stored {len(text_chunks)} vectors in FAISS.\")\n",
                "    \n",
                "    def search(self, query: str, k=3) -> List[str]:\n",
                "        # 1. Embed Query\n",
                "        # (Add instruction prefix if using BGE models)\n",
                "        query_text = f\"Represent this sentence for searching relevant passages: {query}\"\n",
                "        query_embedding = embedder.encode([query_text], convert_to_numpy=True, normalize_embeddings=True)\n",
                "        \n",
                "        # 2. Search\n",
                "        D, I = self.index.search(query_embedding, k)\n",
                "        \n",
                "        results = []\n",
                "        for idx in I[0]:\n",
                "            if idx >= 0 and idx < len(self.chunks):\n",
                "                results.append(self.chunks[idx])\n",
                "        return results\n",
                "\n",
                "# Initialize Store\n",
                "vector_store = VectorStore(dimension=embedder.get_sentence_embedding_dimension())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Chat with Ollama\n",
                "\n",
                "We query your local Ollama instance, providing the retrieved context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_answer(query: str, context_chunks: List[str]):\n",
                "    \"\"\"\n",
                "    Ask Ollama to answer based ONLY on the context.\n",
                "    Replicates the exact System Prompt from the backend.\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Construct Context Block\n",
                "    context_text = \"\\n\\n\".join([f\"[Source {i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)])\n",
                "    \n",
                "    # 2. System Prompt (The \"Brain\" rules)\n",
                "    system_prompt = \"\"\"You are a helpful AI assistant that answers questions based ONLY on the provided source documents.\n",
                "\n",
                "CRITICAL RULES:\n",
                "1. Answer based ONLY on the context provided below. NEVER use external knowledge or assumptions.\n",
                "2. If the information is not explicitly stated in the sources, say \"I cannot find this information in the provided documents.\"\n",
                "3. When citing information, use the exact source reference format [Source X].\n",
                "4. Keep answers factual and accurate.\"\"\"\n",
                "\n",
                "    # 3. User Prompt\n",
                "    final_prompt = f\"\"\"Context from documents:\n",
                "{context_text}\n",
                "\n",
                "---\n",
                "\n",
                "Question: {query}\n",
                "\n",
                "Please provide a comprehensive answer based on the sources above.\"\"\"\n",
                "\n",
                "    # 4. Call Ollama API\n",
                "    payload = {\n",
                "        \"model\": OLLAMA_MODEL,\n",
                "        \"prompt\": final_prompt,\n",
                "        \"system\": system_prompt,\n",
                "        \"stream\": False,\n",
                "        \"options\": {\"temperature\": 0.7}\n",
                "    }\n",
                "    \n",
                "    try:\n",
                "        response = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", json=payload)\n",
                "        if response.status_code == 200:\n",
                "            return response.json()['response']\n",
                "        else:\n",
                "            return f\"âŒ Ollama Error: {response.text}\"\n",
                "    except Exception as e:\n",
                "        return f\"âŒ Connection Failed: is Ollama running? {e}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Bonus: Summarization & Audio (TTS)\n",
                "\n",
                "We can also use the model to summarize the document and read the answer aloud using **gTTS** (Google Text-to-Speech)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# NEW: Summarization Function\n",
                "def summarize_document(context_chunks: List[str]):\n",
                "    \"\"\"\n",
                "    Generate a summary of the document based on the provided chunks.\n",
                "    \"\"\"\n",
                "    # 1. Construct Context Block (Limit to first few chunks to fit context if needed)\n",
                "    # For this demo, we'll use the first 5 chunks or all of them if less.\n",
                "    summary_context = \"\\n\\n\".join(context_chunks[:5]) \n",
                "    \n",
                "    prompt = f\"\"\"Summarize the following document text in a concise and informative paragraph:\n",
                "\n",
                "{summary_context}\n",
                "\n",
                "Summary:\"\"\"\n",
                "\n",
                "    payload = {\n",
                "        \"model\": OLLAMA_MODEL,\n",
                "        \"prompt\": prompt,\n",
                "        \"stream\": False,\n",
                "        \"options\": {\"temperature\": 0.5}\n",
                "    }\n",
                "    \n",
                "    try:\n",
                "        response = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", json=payload)\n",
                "        if response.status_code == 200:\n",
                "            return response.json()['response']\n",
                "        else:\n",
                "            return f\"âŒ Ollama Error: {response.text}\"\n",
                "    except Exception as e:\n",
                "        return f\"âŒ Connection Failed: is Ollama running? {e}\"\n",
                "\n",
                "# NEW: Text-to-Speech Function\n",
                "def speak_text(text: str):\n",
                "    \"\"\"\n",
                "    Convert text to speech and play it in the notebook.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        from gTTS import gTTS\n",
                "        from IPython.display import Audio, display\n",
                "        import io\n",
                "        \n",
                "        print(\"ðŸ”Š Generating Audio...\")\n",
                "        tts = gTTS(text, lang='en')\n",
                "        \n",
                "        # Save to a bytes buffer instead of a file\n",
                "        buffer = io.BytesIO()\n",
                "        tts.write_to_fp(buffer)\n",
                "        buffer.seek(0)\n",
                "        \n",
                "        # Display Audio widget\n",
                "        display(Audio(buffer.read(), autoplay=True))\n",
                "        \n",
                "    except ImportError:\n",
                "        print(\"âŒ gTTS not installed. Run `!pip install gTTS` to enable audio.\")\n",
                "    except Exception as e:\n",
                "        print(f\"âŒ Audio generation failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§ª Interactive Demo Loop\n",
                "\n",
                "Run the cell below to upload a file (by path) and chat with it!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸ¤– AI RAG Demo Ready!\n",
                        "----------------------\n",
                        "â³ Loading Doctr OCR model... (this may take a moment)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "d:\\NoteBook LLM\\venv\\Lib\\site-packages\\doctr\\models\\utils\\pytorch.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
                        "  state_dict = torch.load(archive_path, map_location=\"cpu\")\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… OCR Model Loaded!\n",
                        "ðŸ” Processing ./documents/Naveen_Resume.pdf...\n",
                        "\n",
                        "ðŸ“„ Validated Text Extraction (2722 chars)\n",
                        "âœ‚ï¸  Created 20 chunks from document.\n",
                        "ðŸ’¾ Stored 20 vectors in FAISS.\n",
                        "\n",
                        "âœ¨ Ready to Chat! Type 'exit' to quit. Type 'summary' to summarize the doc.\n"
                    ]
                }
            ],
            "source": [
                "print(\"ðŸ¤– AI RAG Demo Ready!\")\n",
                "print(\"----------------------\")\n",
                "\n",
                "# 1. Get File\n",
                "file_path = \"./documents/Naveen_Resume.pdf\"\n",
                "\n",
                "if os.path.exists(file_path):\n",
                "    # 2. Extract\n",
                "    text = extract_text_from_pdf(file_path)\n",
                "    print(f\"\\nðŸ“„ Validated Text Extraction ({len(text)} chars)\")\n",
                "    \n",
                "    # 3. Chunk\n",
                "    chunks = chunk_text(text)\n",
                "    \n",
                "    # 4. Index\n",
                "    vector_store = VectorStore(dimension=embedder.get_sentence_embedding_dimension())\n",
                "    vector_store.add_documents(chunks)\n",
                "    \n",
                "    print(\"\\nâœ¨ Ready to Chat! Type 'exit' to quit. Type 'summary' to summarize the doc.\")\n",
                "    \n",
                "    while True:\n",
                "        query = input(\"\\nâ“ Ask a question: \")\n",
                "        if query.lower() in ['exit', 'quit']:\n",
                "            break\n",
                "        \n",
                "        answer = \"\"\n",
                "        \n",
                "        if query.lower() == 'summary':\n",
                "             print(\"\\nðŸ“ Generating Summary...\")\n",
                "             answer = summarize_document(chunks)\n",
                "             print(\"\\n\" + answer + \"\\n\")\n",
                "             print(\"-\" * 50)\n",
                "             speak_text(answer)\n",
                "             continue\n",
                "            \n",
                "        # 5. Retrieve\n",
                "        relevant_chunks = vector_store.search(query, k=3)\n",
                "        print(f\"   (Found {len(relevant_chunks)} relevant context chunks)\")\n",
                "        \n",
                "        # 6. Answer\n",
                "        print(\"\\nðŸ¤– Generating Answer...\")\n",
                "        answer = generate_answer(query, relevant_chunks)\n",
                "        print(\"\\n\" + answer + \"\\n\")\n",
                "        print(\"-\" * 50)\n",
                "        \n",
                "        # 7. Audio\n",
                "        speak_text(answer)\n",
                "\n",
                "else:\n",
                "    print(\"âŒ File not found. Please check the path.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
