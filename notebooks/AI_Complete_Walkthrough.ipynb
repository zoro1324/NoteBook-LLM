{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ§  AI Pipeline Walkthrough: From Document to Insight\n",
                "\n",
                "This notebook demonstrates the exact AI workflow used in the **NotebookLLM Clone** application. It runs standalone (without the Django backend) but uses the **same libraries and logic** to show how your data is processed.\n",
                "\n",
                "### ğŸš€ The Pipeline\n",
                "1.  **Ingestion & OCR**: Extract text from PDFs using **Doctr** (Deep Learning OCR).\n",
                "2.  **Chunking**: Split text into manageable overlapping segments.\n",
                "3.  **Embeddings**: Convert text to vectors using **SentenceTransformers** (`BAAI/bge-small-en-v1.5`).\n",
                "4.  **Vector Store**: Index vectors using **FAISS** for fast similarity search.\n",
                "5.  **Generation**: Retrieve context and answer questions using **Ollama** (`phi3:mini` or configured model)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Using Device: CPU\n"
                    ]
                }
            ],
            "source": [
                "# ğŸ“¦ Imports & Configuration\n",
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import numpy as np\n",
                "import faiss\n",
                "import requests\n",
                "import json\n",
                "from pathlib import Path\n",
                "from typing import List, Tuple, Dict\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# âš™ï¸ Configuration (Mirroring backend settings)\n",
                "EMBEDDING_MODEL_NAME = 'BAAI/bge-small-en-v1.5'\n",
                "OLLAMA_BASE_URL = 'http://localhost:11434'\n",
                "OLLAMA_MODEL = 'phi3:mini'  # Change if you use a different model in Ollama\n",
                "CHUNK_SIZE = 512\n",
                "CHUNK_OVERLAP = 50\n",
                "\n",
                "# Check for GPU\n",
                "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"âœ… Using Device: {DEVICE.upper()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Document Processing (OCR)\n",
                "\n",
                "We use **Doctr** (Document Text Recognition) to extract text. This is a powerful Deep Learning-based OCR engine that handles complex layouts better than standard text extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize OCR Model (Cached)\n",
                "ocr_model = None\n",
                "\n",
                "def get_ocr_model():\n",
                "    global ocr_model\n",
                "    if ocr_model is None:\n",
                "        print(\"â³ Loading Doctr OCR model... (this may take a moment)\")\n",
                "        try:\n",
                "            from doctr.models import ocr_predictor\n",
                "            ocr_model = ocr_predictor(pretrained=True).to(DEVICE)\n",
                "            print(\"âœ… OCR Model Loaded!\")\n",
                "        except ImportError:\n",
                "            print(\"âŒ Doctr not installed. Please install it to use OCR features.\")\n",
                "    return ocr_model\n",
                "\n",
                "def extract_text_from_pdf(file_path: str) -> str:\n",
                "    \"\"\"Extract text from PDF using Doctr (matches backend strategy)\"\"\"\n",
                "    model = get_ocr_model()\n",
                "    if not model:\n",
                "        return \"[Error: OCR Unavailable]\"\n",
                "\n",
                "    print(f\"ğŸ” Processing {file_path}...\")\n",
                "    from doctr.io import DocumentFile\n",
                "    \n",
                "    try:\n",
                "        # Load PDF\n",
                "        doc = DocumentFile.from_pdf(file_path)\n",
                "        # Run Inference\n",
                "        result = model(doc)\n",
                "        \n",
                "        # Reconstruct Text\n",
                "        full_text = []\n",
                "        for page_idx, page in enumerate(result.pages):\n",
                "            page_text = []\n",
                "            for block in page.blocks:\n",
                "                for line in block.lines:\n",
                "                    line_text = \" \".join([word.value for word in line.words])\n",
                "                    page_text.append(line_text)\n",
                "            \n",
                "            # Add page marker\n",
                "            full_text.append(f\"--- Page {page_idx + 1} ---\\n\" + \"\\n\".join(page_text))\n",
                "        \n",
                "        return \"\\n\\n\".join(full_text)\n",
                "    except Exception as e:\n",
                "        return f\"âŒ Error extracting text: {str(e)}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Semantic Chunking\n",
                "\n",
                "LLMs have context limits. We split the text into overlapping chunks to ensure we can retrieve just the relevant parts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:\n",
                "    \"\"\"Split text into overlapping chunks\"\"\"\n",
                "    words = text.split()\n",
                "    chunks = []\n",
                "    \n",
                "    current_chunk = []\n",
                "    current_len = 0\n",
                "    \n",
                "    for word in words:\n",
                "        word_len = len(word) + 1\n",
                "        if current_len + word_len > chunk_size and current_chunk:\n",
                "            chunks.append(\" \".join(current_chunk))\n",
                "            # Overlap: keep last 'overlap' words for next chunk\n",
                "            current_chunk = current_chunk[-overlap:]\n",
                "            current_len = sum(len(w) + 1 for w in current_chunk)\n",
                "        \n",
                "        current_chunk.append(word)\n",
                "        current_len += word_len\n",
                "    \n",
                "    if current_chunk:\n",
                "        chunks.append(\" \".join(current_chunk))\n",
                "    \n",
                "    print(f\"âœ‚ï¸  Created {len(chunks)} chunks from document.\")\n",
                "    return chunks"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Embeddings & Vector Store (RAG Core)\n",
                "\n",
                "We convert chunks into **Vector Embeddings** (lists of numbers representing meaning) and store them in **FAISS**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â³ Loading Embedding Model: BAAI/bge-small-en-v1.5...\n",
                        "âœ… Embedding Model Loaded!\n"
                    ]
                }
            ],
            "source": [
                "# Load Embedding Model\n",
                "print(f\"â³ Loading Embedding Model: {EMBEDDING_MODEL_NAME}...\")\n",
                "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
                "print(\"âœ… Embedding Model Loaded!\")\n",
                "\n",
                "class VectorStore:\n",
                "    def __init__(self, dimension=384):\n",
                "        # Using FAISS IndexFlatIP (Inner Product) for Cosine Similarity\n",
                "        # (Requires normalized vectors)\n",
                "        self.index = faiss.IndexFlatIP(dimension)\n",
                "        self.chunks = [] # Store actual text to retrieve later\n",
                "    \n",
                "    def add_documents(self, text_chunks: List[str]):\n",
                "        # 1. Embed\n",
                "        embeddings = embedder.encode(text_chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
                "        \n",
                "        # 2. Add to Index\n",
                "        self.index.add(embeddings)\n",
                "        self.chunks.extend(text_chunks)\n",
                "        print(f\"ğŸ’¾ Stored {len(text_chunks)} vectors in FAISS.\")\n",
                "    \n",
                "    def search(self, query: str, k=3) -> List[str]:\n",
                "        # 1. Embed Query\n",
                "        # (Add instruction prefix if using BGE models)\n",
                "        query_text = f\"Represent this sentence for searching relevant passages: {query}\"\n",
                "        query_embedding = embedder.encode([query_text], convert_to_numpy=True, normalize_embeddings=True)\n",
                "        \n",
                "        # 2. Search\n",
                "        D, I = self.index.search(query_embedding, k)\n",
                "        \n",
                "        results = []\n",
                "        for idx in I[0]:\n",
                "            if idx >= 0 and idx < len(self.chunks):\n",
                "                results.append(self.chunks[idx])\n",
                "        return results\n",
                "\n",
                "# Initialize Store\n",
                "vector_store = VectorStore(dimension=embedder.get_sentence_embedding_dimension())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Chat with Ollama\n",
                "\n",
                "We query your local Ollama instance, providing the retrieved context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_answer(query: str, context_chunks: List[str]):\n",
                "    \"\"\"\n",
                "    Ask Ollama to answer based ONLY on the context.\n",
                "    Replicates the exact System Prompt from the backend.\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. Construct Context Block\n",
                "    context_text = \"\\n\\n\".join([f\"[Source {i+1}]: {chunk}\" for i, chunk in enumerate(context_chunks)])\n",
                "    \n",
                "    # 2. System Prompt (The \"Brain\" rules)\n",
                "    system_prompt = \"\"\"You are a helpful AI assistant that answers questions based ONLY on the provided source documents.\n",
                "\n",
                "CRITICAL RULES:\n",
                "1. Answer based ONLY on the context provided below. NEVER use external knowledge or assumptions.\n",
                "2. If the information is not explicitly stated in the sources, say \"I cannot find this information in the provided documents.\"\n",
                "3. When citing information, use the exact source reference format [Source X].\n",
                "4. Keep answers factual and accurate.\"\"\"\n",
                "\n",
                "    # 3. User Prompt\n",
                "    final_prompt = f\"\"\"Context from documents:\n",
                "{context_text}\n",
                "\n",
                "---\n",
                "\n",
                "Question: {query}\n",
                "\n",
                "Please provide a comprehensive answer based on the sources above.\"\"\"\n",
                "\n",
                "    # 4. Call Ollama API\n",
                "    payload = {\n",
                "        \"model\": OLLAMA_MODEL,\n",
                "        \"prompt\": final_prompt,\n",
                "        \"system\": system_prompt,\n",
                "        \"stream\": False,\n",
                "        \"options\": {\"temperature\": 0.7}\n",
                "    }\n",
                "    \n",
                "    try:\n",
                "        response = requests.post(f\"{OLLAMA_BASE_URL}/api/generate\", json=payload)\n",
                "        if response.status_code == 200:\n",
                "            return response.json()['response']\n",
                "        else:\n",
                "            return f\"âŒ Ollama Error: {response.text}\"\n",
                "    except Exception as e:\n",
                "        return f\"âŒ Connection Failed: is Ollama running? {e}\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ§ª Interactive Demo Loop\n",
                "\n",
                "Run the cell below to upload a file (by path) and chat with it!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ¤– AI RAG Demo Ready!\n",
                        "----------------------\n",
                        "ğŸ” Processing ./documents/Naveen_Resume.pdf...\n",
                        "\n",
                        "ğŸ“„ Validated Text Extraction (2722 chars)\n",
                        "âœ‚ï¸  Created 20 chunks from document.\n",
                        "ğŸ’¾ Stored 20 vectors in FAISS.\n",
                        "\n",
                        "âœ¨ Ready to Chat! Type 'exit' to quit.\n",
                        "   (Found 3 relevant context chunks)\n",
                        "\n",
                        "ğŸ¤– Generating Answer...\n",
                        "\n",
                        "The name of the person mentioned in these LinkedIn profiles is Naveen Kumar G, as stated in [Source 1] and [Source 2]. They describe themselves as an Al and Full-Stack Developer proficient in various technologies such as Python, Django, React, and PostgreSQL. Additionally, they have expertise in machine learning, computer vision, and IoT (Internet of Things). The educational background shared across both sources includes a Bachelor's degree in Technology with specialization in Artifics Inteligence And Data Science from the KGISL Institute of Technology, Coimbatore, obtained in May 2028. Furthermore, they have also completed their Higher Secondary Certificate (HSC) and Matric Hr Sec qualifications as per [Source 1] with respective completion dates provided for each educational milestone: HSC - April 2024, Sree Dharmasastha Matric Hr Sec School - May 2022. The LinkedIn profiles also note skills in programming languages and various technologies pertinent to web development and data science roles.\n",
                        "\n",
                        "--------------------------------------------------\n",
                        "   (Found 3 relevant context chunks)\n",
                        "\n",
                        "ğŸ¤– Generating Answer...\n",
                        "\n",
                        "Naveen Kumar is proficient in Python, Django (backend), React and PostgreSQL; skilled in ML/CV/IoT for scalable real-time applications with strong problem solving, creative thinking, leadership abilities [Source 2]. He completed Bachelor of Technology at GISL Institute specializing in AI and Data Science. Has an HSC from Sree Dharmasastha Matric Hr Sec School; holds a secondary school certificate (SSLC) as well [Sources 1 & 3].\n",
                        "\n",
                        "--------------------------------------------------\n",
                        "   (Found 3 relevant context chunks)\n",
                        "\n",
                        "ğŸ¤– Generating Answer...\n"
                    ]
                }
            ],
            "source": [
                "print(\"ğŸ¤– AI RAG Demo Ready!\")\n",
                "print(\"----------------------\")\n",
                "\n",
                "# 1. Get File\n",
                "file_path = \"./documents/Naveen_Resume.pdf\"\n",
                "\n",
                "if os.path.exists(file_path):\n",
                "    # 2. Extract\n",
                "    text = extract_text_from_pdf(file_path)\n",
                "    print(f\"\\nğŸ“„ Validated Text Extraction ({len(text)} chars)\")\n",
                "    \n",
                "    # 3. Chunk\n",
                "    chunks = chunk_text(text)\n",
                "    \n",
                "    # 4. Index\n",
                "    vector_store = VectorStore(dimension=embedder.get_sentence_embedding_dimension())\n",
                "    vector_store.add_documents(chunks)\n",
                "    \n",
                "    print(\"\\nâœ¨ Ready to Chat! Type 'exit' to quit.\")\n",
                "    \n",
                "    while True:\n",
                "        query = input(\"\\nâ“ Ask a question: \")\n",
                "        if query.lower() in ['exit', 'quit']:\n",
                "            break\n",
                "            \n",
                "        # 5. Retrieve\n",
                "        relevant_chunks = vector_store.search(query, k=3)\n",
                "        print(f\"   (Found {len(relevant_chunks)} relevant context chunks)\")\n",
                "        \n",
                "        # 6. Answer\n",
                "        print(\"\\nğŸ¤– Generating Answer...\")\n",
                "        answer = generate_answer(query, relevant_chunks)\n",
                "        print(\"\\n\" + answer + \"\\n\")\n",
                "        print(\"-\" * 50)\n",
                "\n",
                "else:\n",
                "    print(\"âŒ File not found. Please check the path.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
